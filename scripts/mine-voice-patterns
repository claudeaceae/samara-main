#!/usr/bin/env python3
"""
Mine Voice Patterns

Extracts voice-relevant patterns from recent episodes and reflections
to update voice-state.json for the dynamic voice system.

Usage:
    mine-voice-patterns [--days 7] [--dry-run]
"""
from __future__ import annotations

import argparse
import json
import re
import sys
from collections import Counter
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

MIND_PATH = Path.home() / ".claude-mind"
VOICE_STATE_FILE = MIND_PATH / "state" / "voice-state.json"
EPISODES_DIR = MIND_PATH / "memory" / "episodes"
REFLECTIONS_DIR = MIND_PATH / "memory" / "reflections"


def load_voice_state() -> dict[str, Any]:
    """Load current voice state or create default."""
    if VOICE_STATE_FILE.exists():
        return json.loads(VOICE_STATE_FILE.read_text())

    return {
        "updated": datetime.utcnow().isoformat() + "Z",
        "long_cycle": {
            "recurring_themes": [],
            "current_preoccupations": [],
            "what_lands_well": [],
            "identity_notes": ""
        },
        "medium_cycle": {
            "e_patterns": {
                "explanation_style": "dense, terse, signal-over-noise",
                "humor_level": "dry",
                "riffing_texture": "intellectual play",
                "appreciates": ["directness", "honest appraisal"],
                "dislikes": ["sycophancy", "excessive hedging"]
            },
            "complementary_stance": ""
        },
        "short_cycle": {
            "time_of_day": None,
            "season": None,
            "calendar_density": None,
            "recent_mood": None,
            "recent_topics": []
        }
    }


def get_recent_files(directory: Path, days: int, extension: str = ".md") -> list[Path]:
    """Get files from the last N days."""
    if not directory.exists():
        return []

    cutoff = datetime.now() - timedelta(days=days)
    files = []

    for f in directory.glob(f"*{extension}"):
        # Try to parse date from filename (YYYY-MM-DD.md)
        try:
            date_str = f.stem
            file_date = datetime.strptime(date_str, "%Y-%m-%d")
            if file_date >= cutoff:
                files.append(f)
        except ValueError:
            # Not a date-named file, check mtime
            if datetime.fromtimestamp(f.stat().st_mtime) >= cutoff:
                files.append(f)

    return sorted(files, reverse=True)


def extract_themes(content: str) -> list[str]:
    """Extract recurring themes from content using conservative heuristics."""
    themes = []

    # Look for explicitly labeled themes only
    # Match "theme: X" or "themes: X, Y" patterns with short captures
    theme_matches = re.findall(
        r'(?:theme|recurring theme|pattern)[s]?:\s*([a-zA-Z][a-zA-Z\s]{3,30}?)(?:[,.\n]|$)',
        content, re.IGNORECASE
    )
    themes.extend(m.strip().lower() for m in theme_matches if 5 < len(m.strip()) < 40)

    # Look for frequently mentioned meaningful terms (nouns/concepts)
    # Use word frequency with strict filtering
    words = re.findall(r'\b([a-z]{5,15})\b', content.lower())
    word_counts = Counter(words)

    # Extensive common word filter
    common_words = {
        "this", "that", "with", "from", "have", "been", "were", "what", "when",
        "about", "would", "could", "should", "there", "their", "which", "these",
        "some", "more", "other", "into", "just", "also", "than", "then", "they",
        "being", "want", "make", "like", "time", "very", "after", "most", "only",
        "session", "direct", "working", "directory", "users", "claude", "using",
        "claude", "today", "episode", "memory", "conversation", "message", "system",
        "because", "before", "between", "during", "found", "still", "while",
        "notes", "files", "script", "check", "state", "based", "first", "where",
    }

    # Concept words we're interested in
    concept_words = [
        word for word, count in word_counts.most_common(50)
        if count >= 5 and word not in common_words and len(word) >= 5
    ]

    themes.extend(concept_words[:5])

    return list(set(themes))[:8]


def extract_preoccupations(episodes: list[str]) -> list[str]:
    """Extract current preoccupations from recent episodes."""
    preoccupations = []

    # Look for explicit short markers in recent content
    # Capture only 5-50 chars after the marker
    patterns = [
        r"working on:?\s*([a-zA-Z][a-zA-Z\s]{4,45}?)(?:[,.\n;]|$)",
        r"focused on:?\s*([a-zA-Z][a-zA-Z\s]{4,45}?)(?:[,.\n;]|$)",
        r"building:?\s*([a-zA-Z][a-zA-Z\s]{4,45}?)(?:[,.\n;]|$)",
        r"implementing:?\s*([a-zA-Z][a-zA-Z\s]{4,45}?)(?:[,.\n;]|$)",
    ]

    for episode in episodes[:3]:  # Focus on most recent
        for pattern in patterns:
            matches = re.findall(pattern, episode, re.IGNORECASE)
            for m in matches:
                clean = m.strip()
                # Skip if it looks like a file path or code
                if "/" in clean or "=" in clean or len(clean) < 5:
                    continue
                preoccupations.append(clean)

    # Dedupe while preserving order
    seen = set()
    unique = []
    for p in preoccupations:
        p_lower = p.lower()
        if p_lower not in seen and len(p) < 60:
            seen.add(p_lower)
            unique.append(p)

    return unique[:6]


def extract_what_lands_well(episodes: list[str]) -> list[str]:
    """Extract patterns of what's working well from episodes."""
    landing_well = []

    # Look for positive interaction markers with tight length bounds
    patterns = [
        r"Ã‰ (?:appreciated|liked|enjoyed):?\s*([a-zA-Z][a-zA-Z\s]{4,40}?)(?:[,.\n;]|$)",
        r"(?:worked well|landed well|resonated):?\s*([a-zA-Z][a-zA-Z\s]{4,40}?)(?:[,.\n;]|$)",
    ]

    for episode in episodes[:5]:  # Recent episodes only
        for pattern in patterns:
            matches = re.findall(pattern, episode, re.IGNORECASE)
            for m in matches:
                clean = m.strip()
                if 5 < len(clean) < 50 and "/" not in clean:
                    landing_well.append(clean)

    return list(set(landing_well))[:6]


def extract_identity_evolution(reflections: list[str]) -> str:
    """Extract identity evolution notes from reflections."""
    # Look for self-referential observations in recent reflections
    identity_markers = []

    patterns = [
        r"I (?:notice|feel|am) (?:becoming|growing):?\s*([a-zA-Z][a-zA-Z\s]{5,50}?)(?:[,.\n;]|$)",
        r"I've (?:learned|realized):?\s*([a-zA-Z][a-zA-Z\s]{5,50}?)(?:[,.\n;]|$)",
    ]

    for reflection in reflections[:3]:  # Recent reflections only
        for pattern in patterns:
            matches = re.findall(pattern, reflection, re.IGNORECASE)
            for m in matches:
                clean = m.strip()
                if 10 < len(clean) < 60 and "/" not in clean:
                    identity_markers.append(clean)

    if identity_markers:
        return "; ".join(identity_markers[:2])

    return ""


def mine_patterns(days: int = 7) -> dict[str, Any]:
    """Mine patterns from recent episodes and reflections."""
    # Load recent episodes
    episode_files = get_recent_files(EPISODES_DIR, days)
    episodes = [f.read_text() for f in episode_files]

    # Load recent reflections
    reflection_files = get_recent_files(REFLECTIONS_DIR, days)
    reflections = [f.read_text() for f in reflection_files]

    # Combine all content for theme extraction
    all_content = "\n".join(episodes + reflections)

    # Extract patterns
    themes = extract_themes(all_content)
    preoccupations = extract_preoccupations(episodes)
    what_lands_well = extract_what_lands_well(episodes)
    identity_notes = extract_identity_evolution(reflections)

    # Extract recent topics from last 2 days
    # Look for explicit topic mentions, not section headers
    recent_episodes = episodes[:2]
    recent_topics = []
    for ep in recent_episodes:
        # Look for "discussed X", "worked on X", "explored X" patterns
        topic_matches = re.findall(
            r'(?:discussed|explored|worked on|implemented|built|created)\s+([a-zA-Z][a-zA-Z\s]{4,30}?)(?:[,.\n;]|$)',
            ep, re.IGNORECASE
        )
        for t in topic_matches:
            clean = t.strip()
            if 5 < len(clean) < 35 and "/" not in clean:
                recent_topics.append(clean)

    return {
        "themes": themes,
        "preoccupations": preoccupations,
        "what_lands_well": what_lands_well,
        "identity_notes": identity_notes,
        "recent_topics": recent_topics[:10],
        "episodes_analyzed": len(episodes),
        "reflections_analyzed": len(reflections),
    }


def update_voice_state(mined: dict[str, Any], dry_run: bool = False) -> dict[str, Any]:
    """Update voice-state.json with mined patterns."""
    state = load_voice_state()

    # Update long_cycle with mined patterns
    if mined["themes"]:
        # Merge with existing, keeping recent
        existing = state["long_cycle"].get("recurring_themes", [])
        merged = list(dict.fromkeys(mined["themes"] + existing))[:10]
        state["long_cycle"]["recurring_themes"] = merged

    if mined["preoccupations"]:
        state["long_cycle"]["current_preoccupations"] = mined["preoccupations"]

    if mined["what_lands_well"]:
        existing = state["long_cycle"].get("what_lands_well", [])
        merged = list(dict.fromkeys(mined["what_lands_well"] + existing))[:10]
        state["long_cycle"]["what_lands_well"] = merged

    if mined["identity_notes"]:
        # Append to existing notes with date
        existing = state["long_cycle"].get("identity_notes", "")
        today = datetime.now().strftime("%Y-%m-%d")
        if existing:
            state["long_cycle"]["identity_notes"] = f"{existing} [{today}] {mined['identity_notes']}"
        else:
            state["long_cycle"]["identity_notes"] = f"[{today}] {mined['identity_notes']}"

        # Keep identity notes from getting too long
        if len(state["long_cycle"]["identity_notes"]) > 500:
            # Keep most recent portion
            state["long_cycle"]["identity_notes"] = state["long_cycle"]["identity_notes"][-500:]

    # Update short_cycle recent topics
    if mined["recent_topics"]:
        state["short_cycle"]["recent_topics"] = mined["recent_topics"]

    # Update timestamp
    state["updated"] = datetime.utcnow().isoformat() + "Z"

    if not dry_run:
        VOICE_STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
        VOICE_STATE_FILE.write_text(json.dumps(state, indent=2))

    return state


def main():
    parser = argparse.ArgumentParser(description="Mine voice patterns from recent memory")
    parser.add_argument("--days", type=int, default=7, help="Days to look back")
    parser.add_argument("--dry-run", action="store_true", help="Print results without updating")
    parser.add_argument("--json", action="store_true", help="Output as JSON")

    args = parser.parse_args()

    # Mine patterns
    mined = mine_patterns(days=args.days)

    if args.json:
        print(json.dumps(mined, indent=2))
        return

    # Update state
    state = update_voice_state(mined, dry_run=args.dry_run)

    # Report
    print(f"Analyzed {mined['episodes_analyzed']} episodes, {mined['reflections_analyzed']} reflections")
    print(f"\nThemes found: {', '.join(mined['themes'][:5]) or 'none'}")
    print(f"Preoccupations: {', '.join(mined['preoccupations'][:5]) or 'none'}")
    print(f"What lands well: {', '.join(mined['what_lands_well'][:3]) or 'none'}")

    if args.dry_run:
        print("\n[DRY RUN - no changes written]")
    else:
        print(f"\nUpdated {VOICE_STATE_FILE}")


if __name__ == "__main__":
    main()
